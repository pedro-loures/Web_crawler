{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH = 4\n",
    "\n",
    "PATH_TO_CORPUS = \"C:\\\\Users\\\\PLour\\\\OneDrive - Universidade Federal de Minas Gerais\\\\01_Estudos\\\\Faculdade\\\\RI\\\\Crawler\\\\Corpus\"\n",
    "\n",
    "# external modules\n",
    "import sys \n",
    "\n",
    "\n",
    "# internal modules\n",
    "import utils as ut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_delay(delay=None):\n",
    "    if not delay:\n",
    "        time.sleep(0.1)\n",
    "    else:\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://exame.com/'\n",
    "page = requests.get(url, auth=('user', 'pass'))\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not allowed in: http://exame-faq.force.com/s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import utils as ut\n",
    "url_queue = {}\n",
    "url_queue_ = {}\n",
    "url_queue_ = ut.add_url_to_queue(soup, url, url_queue_, _url_depth=DEPTH)\n",
    "url_recusado = {}\n",
    "_previous_url = ''\n",
    "allowed, delay, robots= True, None, None\n",
    "request_time_list = []\n",
    "robots_time_list = []\n",
    "\n",
    "for i, url in enumerate(url_queue_):\n",
    "    \n",
    "    if i > 100:\n",
    "        break\n",
    "\n",
    "    _url_prefix = url.split('/', maxsplit=1)[0] \n",
    "    _previous_url_prefix = _previous_url.split('/', maxsplit=1)[0]\n",
    "    \n",
    "    domain, url = ut.process_url(url)\n",
    "    \n",
    "    lap_time = time.time()\n",
    "    \n",
    "    if _url_prefix != _previous_url_prefix:\n",
    "        # print(\"awake\")\n",
    "        allowed, robots = ut.check_robots(domain)\n",
    "    else:\n",
    "        allowed, delay = ut.check_robots(domain, robots)\n",
    "        # print(\"sleep\")\n",
    "        sleep_delay(delay)\n",
    "        \n",
    "    robots_time_list.append(time.time() - lap_time)\n",
    "\n",
    "    if not allowed:\n",
    "        print('not allowed in: ' + url)\n",
    "        continue\n",
    "\n",
    "    state = ['NADA', 'PAGE', 'SOUP', 'QUEUE', 'OVER']\n",
    "    iter_state = 0\n",
    "    try:\n",
    "        iter_state = 0\n",
    "        lap_time = time.time()\n",
    "        page = requests.get(url, auth=('user', 'pass'))\n",
    "        request_time_list.append(time.time() - lap_time)\n",
    "        iter_state = 1\n",
    "\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        iter_state = 2\n",
    "        old_len = len(url_queue)\n",
    "    except: \n",
    "        url_recusado[url] = url\n",
    "        print(\"\")\n",
    "        print(\"[STATE: \" + state[iter_state] + \"] couldn't access: \" + url)\n",
    "        request_time_list.append(time.time() - lap_time)\n",
    "\n",
    "    url_queue = ut.add_url_to_queue(soup, url, url_queue, _url_depth=DEPTH)\n",
    "    _previous_url = url[7:]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.1901253509521483, 0.30202245240164277)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_request_time_list = np.array(request_time_list)\n",
    "np_robots_time_list = np.array(robots_time_list)\n",
    "np_request_time_list.mean(), np_robots_time_list.mean()\n",
    "#np_request_time_list.shape, np_robots_time_list.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 207, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_queue), len(url_queue_), len(url_recusado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cccfb4b407a221436763924cf34bbb75b5aa3101f3487ca2e48c1456f4633ce1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('web_crawler')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
